{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df0f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the model\n",
    "model = KeyedVectors.load_word2vec_format('C:/Users/mouni/Downloads/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec')\n",
    "\n",
    "# Now you can use the model\n",
    "vector = model['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d6862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from utils import (cosine_similarity, get_dict,\n",
    "                   process_tweet)\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643b774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 999994\n",
      "Vector dimension: 300\n",
      "Total size: (999994, 300)\n",
      "Vector for 'word': [-0.0078  0.0075  0.0203  0.0991  0.0315 -0.0284  0.144   0.1352  0.0612\n",
      "  0.0649 -0.0026 -0.0589  0.0478 -0.0337 -0.0233  0.0469 -0.0836 -0.0053\n",
      " -0.0024  0.0059 -0.097  -0.0849 -0.1521  0.0512 -0.0095 -0.0535  0.0353\n",
      "  0.101  -0.0254  0.0507  0.1445  0.0814  0.0186 -0.0705 -0.005   0.1171\n",
      " -0.0342 -0.0678  0.0331  0.124   0.0532  0.1233 -0.0551 -0.0339  0.0393\n",
      "  0.042   0.0718  0.0245  0.0014  0.0358  0.0248  0.0603 -0.629   0.0427\n",
      " -0.1074 -0.0039  0.021  -0.2132  0.0053 -0.048   0.1041  0.0203 -0.1299\n",
      "  0.0821  0.0068 -0.1315  0.1093  0.0268  0.0453  0.086   0.0879 -0.081\n",
      " -0.0609 -0.0699  0.0355  0.1278  0.0056 -0.1478 -0.0338  0.053   0.042\n",
      "  0.0057 -0.1486 -0.254  -0.0338  0.031   0.1076  0.1301  0.2479 -0.0424\n",
      "  0.0194  0.0241  0.0553 -0.021   0.0199  0.1346 -0.0271 -0.0165  0.0626\n",
      " -0.0277 -0.1648  0.1067  0.0814  0.0561  0.02    0.0851  0.077   0.1487\n",
      "  0.0184  0.0639  0.2449  0.0458  0.0254 -0.0777 -0.2763  0.1088 -0.0322\n",
      " -0.0968  0.0203 -0.3275  0.0119  0.0009  0.1245 -0.0313  0.0185  0.2253\n",
      " -0.1137  0.1194 -0.0373  0.1251  0.0708 -0.0121  0.1238 -0.0329  0.001\n",
      " -0.0168  0.247   0.038  -0.0187 -0.066   0.0496 -0.056  -0.0356  0.3056\n",
      "  0.1101 -0.1     0.0857  0.0666 -0.0783 -0.0246  0.0656  0.047   0.0117\n",
      " -0.0288  0.0195  0.0888 -0.0011  0.0498  0.1543 -0.0046 -0.0119 -0.0712\n",
      " -0.0052  0.0329 -0.0581 -0.2167  0.0853 -0.0082 -0.0709  0.1088 -0.0385\n",
      "  0.0906 -0.0351  0.0871  0.18    0.003   0.2298 -0.0597  0.0496  0.064\n",
      "  0.0209  0.1061 -0.016  -0.066  -0.0098 -0.194   0.0173 -0.109  -0.1043\n",
      "  0.0094 -0.058  -0.0052  0.0969  0.0276 -0.1534  0.0294  0.0912  0.0732\n",
      "  0.1945  0.2065 -0.1425  0.0855 -0.047   0.0231  0.0248  0.0069  0.0312\n",
      " -0.0188  0.0057  0.0325  0.022  -0.0964 -0.0249  0.0964 -0.0164  0.0064\n",
      " -0.0219 -0.1127 -0.2472 -0.1389 -0.0292  0.1162  0.083  -0.0045 -0.178\n",
      " -0.2347  0.0411 -0.0733 -0.0837 -0.1786 -0.1685  0.0646  0.349  -0.156\n",
      "  0.1804 -0.0815  0.1246 -0.0582 -0.2991 -0.1353 -0.1466  0.0152 -0.0964\n",
      "  0.0934 -0.0209  0.0703 -0.0528  0.0049 -0.0321  0.3874  0.0239 -0.0555\n",
      "  0.0211 -0.0027 -0.0128 -0.0043 -0.015  -0.0504 -0.0897 -0.0163  0.0572\n",
      "  0.163  -0.003  -0.0843 -0.4085  0.0635 -0.0108 -0.0066 -0.1859  0.0913\n",
      " -0.0782 -0.026  -0.0953  0.0216 -0.0513 -0.0169  0.024  -0.129   0.038\n",
      "  0.07    0.0363  0.0553  0.1492 -0.0207 -0.0066 -0.0401  0.0789 -0.0303\n",
      " -0.0041 -0.0201  0.0926 -0.0591 -0.1504  0.103  -0.0249  0.0168 -0.0684\n",
      "  0.118   0.0308 -0.0521]\n",
      "Length of vector: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(model.key_to_index)}\")\n",
    "print(f\"Vector dimension: {model.vector_size}\")\n",
    "print(f\"Total size: {model.vectors.shape}\")\n",
    "\n",
    "# Example of getting a vector for a word\n",
    "vector = model['word']\n",
    "print(f\"Vector for 'word': {vector}\")\n",
    "print(f\"Length of vector: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a281d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Updated preprocessing function to remove unwanted words\n",
    "def preprocess_word(word):\n",
    "    # Remove stock market tickers like $GE\n",
    "    if re.search(r'\\$\\w*', word):\n",
    "        return None\n",
    "    # Remove old style retweet text \"RT\"\n",
    "    if re.search(r'^RT[\\s]+', word):\n",
    "        return None\n",
    "    # Remove hyperlinks and \"www\"\n",
    "    if re.search(r'https?:\\/\\/.*[\\r\\n]*', word) or 'www' in word:\n",
    "        return None\n",
    "    # Remove hashtags (only removing the hash # sign from the word)\n",
    "    if re.search(r'#', word):\n",
    "        return None\n",
    "    # Remove words containing special characters\n",
    "    if re.search(r'[^a-zA-Z0-9]', word):\n",
    "        return None\n",
    "    return word\n",
    "\n",
    "# Get the vocabulary of the embeddings\n",
    "vocab = list(model.index_to_key)\n",
    "\n",
    "# Apply preprocessing to each word in the vocabulary and store their original vectors\n",
    "preprocessed_embeddings = {}\n",
    "for word in vocab:\n",
    "    preprocessed_word = preprocess_word(word)\n",
    "    if preprocessed_word:  # Only add non-None words\n",
    "        preprocessed_embeddings[preprocessed_word] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e17ea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 763569\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(preprocessed_embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab605b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7th and 8th synonyms using Manhattan distance: ['fool', 'fools', 'idiot', 'stupid', 'foolish', 'deluded', 'misleader']\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "# Define the Manhattan distance function\n",
    "def manhattan_distance(v1, v2):\n",
    "    return np.sum(np.abs(v1 - v2))\n",
    "\n",
    "\n",
    "# def euclidean_distance(v1, v2):\n",
    "#     return np.linalg.norm(v1 - v2)\n",
    "\n",
    "# Define the Cosine similarity function\n",
    "#nelghouuuh sahbi\n",
    "# def cosine_similarity(v1, v2):\n",
    "#     dot_product = np.dot(v1, v2)\n",
    "#     norm_v1 = np.linalg.norm(v1)\n",
    "#     norm_v2 = np.linalg.norm(v2)\n",
    "#     return dot_product / (norm_v1 * norm_v2)\n",
    "# Function to get top n similar words using Manhattan distance\n",
    "\n",
    "\n",
    "\n",
    "def get_top_n_similar_words_manhattan(input_vector, embeddings, n=3):\n",
    "    distances = []\n",
    "\n",
    "    for word, word_vector in embeddings.items():\n",
    "        distance = manhattan_distance(input_vector, word_vector)\n",
    "        distances.append((distance, word))\n",
    "    \n",
    "    top_n_words = heapq.nsmallest(n, distances, key=lambda x: x[0])  # Smallest distances for most similar\n",
    "    return [word for _, word in top_n_words]\n",
    "\n",
    "# Example input vector (this should be replaced with an actual word vector)\n",
    "input_word = \"fool\"\n",
    "input_vector = model[input_word]\n",
    "\n",
    "# Get top 8 similar words using Manhattan distance\n",
    "if(input_word in preprocessed_embeddings):\n",
    "        top_n_words_manhattan = get_top_n_similar_words_manhattan(input_vector, preprocessed_embeddings,n=7)\n",
    "\n",
    "# Extract the 7th and 8th most similar words\n",
    "# if len(top_n_words_manhattan) >= 8:\n",
    "#     seventh_eighth_words_manhattan = top_n_words_manhattan[0:7]\n",
    "# else:\n",
    "#     seventh_eighth_words_manhattan = top_n_words_manhattan[6:]  # In case there are fewer than 8 words\n",
    "\n",
    "print(\"7th and 8th synonyms using Manhattan distance:\", top_n_words_manhattan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c075d",
   "metadata": {},
   "source": [
    "euclidean_distance:\n",
    "['happy', 'unhappy', 'happier', 'contented', 'glad', 'pleased', 'sad']\n",
    "['angry', 'enraged', 'frustrated', 'irate', 'annoyed', 'indignant', 'outraged']\n",
    "['sad', 'pathetic', 'depressing', 'tragic', 'unhappy', 'sorrowful', 'unfortunate']\n",
    "['wisdom', 'knowledge', 'wise', 'Wisdom', 'insight', 'sensibleness', 'virtue']\n",
    "['fool', 'fools', 'idiot', 'misleader', 'trick', 'dumbshit', 'knowitall']\n",
    "['skillful', 'skilful', 'skilled', 'deft', 'crafty', 'masterful', 'adept']\n",
    "\n",
    "\n",
    "Manhattan distance:\n",
    "['happy',        'unhappy', 'happier', 'glad', 'contented', 'grateful', 'hapy']\n",
    "['angry',       'enraged', 'frustrated', 'irate', 'annoyed', 'furious', 'angered']\n",
    "['sad', '            pathetic', 'tragic', 'depressing', 'unhappy', 'sorrowful', 'happy']\n",
    "['wisdom',       'wise', 'knowledge', 'virtue', 'sensibleness', 'Wisdom', 'wiseness']\n",
    " ['fool',      'fools', 'idiot', 'stupid', 'foolish', 'deluded', 'misleader']\n",
    " ['skillful', 'skilful', 'skilled', 'deft', 'crafty', 'adroit', 'masterful']\n",
    "\n",
    "\n",
    " Top 7 synonyms for 'skillful' using Manhattan distance and LSH: ['skillful', 'skilful', 'masterful', 'clever', 'adept', 'skillfully', 'perceptive']\n",
    "Top 7 synonyms for 'fool' using Manhattan distance and LSH: ['fool', 'fools', 'nobhead', 'diguise', 'knowitall', 'dummie', 'suckup']\n",
    "Top 7 synonyms for 'wisdom' using Manhattan distance and LSH: ['wisdom', 'wise', 'sagacity', 'solicitousness', 'stumblings', 'grandure', 'scrutable']\n",
    "Top 7 synonyms for 'sad' using Manhattan distance and LSH: ['sad', 'horrible', 'strange', 'troubling', 'scary', 'obessed', 'unsettling']\n",
    "Top 7 synonyms for 'angry' using Manhattan distance and LSH: ['angry', 'indignant', 'vengeful', 'distrubed', 'strident', 'disturbed', 'wose']\n",
    "Top 7 synonyms for 'happy' using Manhattan distance and LSH: ['happy', 'happier', 'embarrassed', 'Liselott', 'Selonians', 'germanised', 'Ebizur']\n",
    "\n",
    "\n",
    "Top 4 synonyms for 'skillful' using Manhattan distance and LSH:\n",
    "['deft', 'crafty', 'adroit', 'masterful']\n",
    "Top 4 synonyms for 'fool' using Manhattan distance and LSH:\n",
    "['idiot', 'foolish', 'daftie', 'overanalyse']\n",
    "Top 4 synonyms for 'wisdom' using Manhattan distance and LSH: \n",
    "['knowledge', 'virtue', 'foresightedness', 'staidness']\n",
    "Top 4 synonyms for 'sad' using Manhattan distance and LSH: \n",
    "['tragic', 'depressing', 'unhappy', 'unforunate']\n",
    "Top 4 synonyms for 'angry' using Manhattan distance and LSH: \n",
    "['enraged', 'frustrated', 'irate', 'furious']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e345763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 synonyms for 'skillful' using Manhattan distance and LSH: ['skillful', 'skilful', 'skilled', 'deft', 'crafty', 'adroit', 'masterful']\n",
      "Top 7 synonyms for 'fool' using Manhattan distance and LSH: ['fool', 'idiot', 'foolish', 'daftie', 'knowitall', 'overanalyse', 'trick']\n",
      "Top 7 synonyms for 'wisdom' using Manhattan distance and LSH: ['wisdom', 'knowledge', 'virtue', 'foolishness', 'foresightedness', 'staidness', 'infallability']\n",
      "Top 7 synonyms for 'sad' using Manhattan distance and LSH: ['sad', 'tragic', 'depressing', 'unhappy', 'unfortunate', 'funny', 'unforunate']\n",
      "Top 7 synonyms for 'angry' using Manhattan distance and LSH: ['angry', 'enraged', 'frustrated', 'irate', 'annoyed', 'furious', 'angered']\n",
      "Top 7 synonyms for 'happy' using Manhattan distance and LSH: ['happy', 'unhappy', 'happier', 'glad', 'contented', 'hapy', 'happpy']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSH:\n",
    "    def __init__(self, num_planes, input_dim, num_universes):\n",
    "        self.num_planes = num_planes\n",
    "        self.input_dim = input_dim\n",
    "        self.num_universes = num_universes\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        self.planes_l = [np.random.normal(size=(input_dim, num_planes)) for _ in range(num_universes)]\n",
    "        \n",
    "        self.hash_tables = []\n",
    "        self.id_tables = []\n",
    "        for _ in range(num_universes):\n",
    "            hash_table = {i: [] for i in range(2 ** num_planes)}\n",
    "            id_table = {i: [] for i in range(2 ** num_planes)}\n",
    "            self.hash_tables.append(hash_table)\n",
    "            self.id_tables.append(id_table)\n",
    "\n",
    "    def hash_value_of_vector(self, v, planes):\n",
    "        dot_product = np.dot(v, planes)\n",
    "        sign_of_dot_product = np.sign(dot_product)\n",
    "        h = sign_of_dot_product >= 0\n",
    "        h = np.squeeze(h)\n",
    "        hash_value = 0\n",
    "        n_planes = planes.shape[1]\n",
    "        for i in range(n_planes):\n",
    "            hash_value += np.power(2, i) * h[i]\n",
    "        hash_value = int(hash_value)\n",
    "        return hash_value\n",
    "\n",
    "    def add_vector(self, vector, label):\n",
    "        for i in range(self.num_universes):\n",
    "            planes = self.planes_l[i]\n",
    "            hash_value = self.hash_value_of_vector(vector, planes)\n",
    "            self.hash_tables[i][hash_value].append(vector)\n",
    "            self.id_tables[i][hash_value].append(label)\n",
    "\n",
    "    def query(self, vector):\n",
    "        candidates = set()\n",
    "        for i in range(self.num_universes):\n",
    "            planes = self.planes_l[i]\n",
    "            hash_value = self.hash_value_of_vector(vector, planes)\n",
    "            if hash_value in self.id_tables[i]:\n",
    "                candidates.update(self.id_tables[i][hash_value])\n",
    "        return list(candidates)\n",
    "\n",
    "# Example function to compute Manhattan distance and find top N similar words\n",
    "def get_top_n_similar_words_manhattan(input_vector, candidate_embeddings, n=7):\n",
    "    distances = {}\n",
    "    for word, vector in candidate_embeddings.items():\n",
    "        distances[word] = np.sum(np.abs(input_vector - vector))\n",
    "    sorted_words = sorted(distances, key=distances.get)[:n]\n",
    "    return sorted_words\n",
    "\n",
    "# Initialize LSH\n",
    "input_dim = model.vector_size\n",
    "num_planes = 10\n",
    "num_universes = 25\n",
    "lsh = LSH(num_planes, input_dim, num_universes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vectors to LSH\n",
    "for word, vector in preprocessed_embeddings.items():\n",
    "    lsh.add_vector(vector, word)\n",
    "\n",
    "# Example input list\n",
    "input_list1 = [\n",
    "    'skillful',\n",
    "    'fool',   \n",
    "    'wisdom',\n",
    "    'sad',\n",
    "    'angry',\n",
    "    'happy',    \n",
    "]\n",
    "\n",
    "# Process each word in the input list\n",
    "for input_word in input_list1:\n",
    "    if input_word in preprocessed_embeddings:\n",
    "        input_vector = preprocessed_embeddings[input_word]\n",
    "        candidate_words = lsh.query(input_vector)\n",
    "        candidate_embeddings = {word: preprocessed_embeddings[word] for word in candidate_words}\n",
    "        top_n_words_manhattan = get_top_n_similar_words_manhattan(input_vector, candidate_embeddings, n=7)\n",
    "        print(f\"Top 7 synonyms for '{input_word}' using Manhattan distance and LSH:\", top_n_words_manhattan)\n",
    "    else:\n",
    "        print(f\"Word '{input_word}' not found in the preprocessed embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "682c017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dimensions(self):\n",
    "        print(f\"Number of hyperplanes (num_planes): {self.num_planes}\")\n",
    "        print(f\"Input vector dimension (input_dim): {self.input_dim}\")\n",
    "        print(f\"Number of universes (num_universes): {self.num_universes}\")\n",
    "        print(f\"Shape of planes in each universe: {self.planes_l[0].shape}\")\n",
    "        print(f\"Number of buckets in each hash table: {2 ** self.num_planes}\")\n",
    "        print(f\"Number of hash tables: {len(self.hash_tables)}\")\n",
    "        for i, (hash_table, id_table) in enumerate(zip(self.hash_tables, self.id_tables)):\n",
    "            print(f\"Universe {i+1}:\")\n",
    "            print(f\"  Number of entries in hash table: {sum(len(bucket) for bucket in hash_table.values())}\")\n",
    "            print(f\"  Number of entries in ID table: {sum(len(bucket) for bucket in id_table.values())}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c9ef3f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSH' object has no attribute 'display_dimensions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lsh\u001b[38;5;241m.\u001b[39mdisplay_dimensions()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LSH' object has no attribute 'display_dimensions'"
     ]
    }
   ],
   "source": [
    "lsh.display_dimensions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
